{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Meaning of words \n",
    "\n",
    "\n",
    "## Distributional hypothesis\n",
    "\n",
    "_\"You shall know a word by the company it keeps\" (Firth, 1957)_\n",
    "\n",
    "The grounding hypothesis of **distributional semantics** is that we consider **language production** (the choice of sequences of words) as a **probabilistic process**, thus we state, that meaning can and should be modeled with some kind of (conditional) probability distributions.\n",
    "\n",
    " \n",
    "-------------------\n",
    "In short:\n",
    "<font color='red'>\n",
    "Meaning of a word = distribution of it's neighbors\n",
    "</font>\n",
    "\n",
    "-------------------\n",
    "\n",
    "## Language modeling\n",
    "\n",
    "A __language model__ is a __probability distribution__ over the __sequence of words__, modeling language (production), thus if the set of words is $w$, then for arbitrary $\\mathbf w = \\langle w_1,\\dots, w_n\\rangle$ ($w_i\\in W$) sequence it defines a $P(\\mathbf w)$ probability. \n",
    "\n",
    "Probability with chain rule:\n",
    "\n",
    "$$P(\\mathbf w)= P(w_1)\\cdot P(w_2 \\vert w_1 )\\cdot P(w_3\\vert w_1, w_2)\\cdot\\dots\\cdot P(w_n\\vert w_1,\\dots, w_{n-1})$$\n",
    "\n",
    "so this means, that for the __modeling__ we need only to give the __conditional probability__ of the __\"continuation\"__, the __next word__, thus for $w$ word and $\\langle w_1,\\dots,w_n\\rangle$ sequence the probability that the next word will be $w$\n",
    "\n",
    "$$P(w ~\\vert ~ w_1,\\dots,w_n)$$\n",
    "\n",
    "There are character based models also, which take the individual characters as units, not the words, and model language as a distribution over sequences of characters (think T9...)\n",
    "\n",
    "\n",
    "\n",
    "**Language modeling is the practical application of the distribution hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \"Don't count, Predict!\"\n",
    "\n",
    "Philosopically the big shift __shift away from the \"count based\" methods__ came with the advent of methods, that try to directly __model__ the __generative process hypothetized behind language__ with the help of __predictive models__ that __approximate__ the __\"next step\"__ in the __generative behavior__.\n",
    "For a more thorough elaboration see [Baroni et al.](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf)\n",
    "\n",
    "The slogan is:\n",
    "\n",
    "**Don't count, predict!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence matters\n",
    "\n",
    "The predictive paradigm explicitly tries to account for the fact that in language, sequence matters.\n",
    "\n",
    "**Illustration:**\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-B32VmhfzIuc/UGYEnvHSE8I/AAAAAAAAAdY/qWn3B3fW_kk/s400/Snarky_meteorite.JPG\" width=45%>\n",
    "\n",
    "Moreover it is also true, that **long term dependencies** are having a strong influence over the semantics of a given phrase.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*hBkRpcC7sMJGrxjXpMiF9Q.png\" width=50%>\n",
    "\n",
    "\"Bag of words\" is essentially blind to this.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- “She only told him that she loved him.” \n",
    "- “She told only him that she loved him.” \n",
    "- “She told him only that she loved him.” \n",
    "- “She told him that only she loved him.” \n",
    "- “She told him that she only loved him.” \n",
    "- “She told him that she loved only him.” \n",
    "\n",
    "The sentences above all have the same BOW representation, wheras for humans they are indeed quite different!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basics of squential data (recap)  \n",
    "\n",
    "**Sequential Data:**\n",
    "\n",
    "The order of the data matters, but the time stamp is irrelevant or it doesn’t matter. (Example: DNA sequence. As you see the concept of time is irrelevant, so the order is not temporal.)\n",
    "\n",
    "\n",
    "**Temporal Sequence:**\n",
    "\n",
    "In addition to the order of data, the time stamp also matters. (Example: Data collected from customers’ shopping behaviour, considering their transaction time stamp as the temporal dimension.)\n",
    "\n",
    "**Time Series:**\n",
    "\n",
    "The data is in order, with a fixed time-difference between occurrence of successive data points. (Example: Time series of the temperature of a surface being recorded every 120 seconds.) \n",
    "\n",
    "([source](https://www.quora.com/What-is-the-difference-between-time-series-and-sequential-data))\n",
    "\n",
    "\n",
    "**There are obvious differences from other datasets:**\n",
    "\n",
    "- The __sequential position__ of the datapoints (commonly represented as dat arrows) is of __paramount__ importance (we are in trouble if we would like to draw a random sample...)\n",
    "- We should __not have__ an __i.i.d. assumption__, so we should be well advised to think, that there is a relationship between successive datapoints\n",
    "- It is always __suspicious__ whether the __[Markov assumption](https://en.wikipedia.org/wiki/Markov_property)__ holds in case of the data. For random walks it does, but it it is rarely the case in practice. (We can try to figure out which \"order\" of Markov property is present, but it is not a clear cut line. (Does the weather of tomorrow depend on today? And yesterday? And the year before? And hundered?...)\n",
    "\n",
    "\n",
    "Or more precisely: language data is a **categorical sequence** that we can try to model either by a **fixed, window like context** or a **full sequence model**.\n",
    "\n",
    "All the approaches below will try to balance these two views.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Rolling window\" models\n",
    "\n",
    "Example for a __rolling window__ approach in case of a __sequential dataset__:\n",
    "\n",
    "<img src=\"https://www.mathworks.com/help/econ/rollingwindow.png\" width=45%>\n",
    "\n",
    "This is most eminently used by: **word2vec** \n",
    "\n",
    "In their paper titled \"Distributed Representations of Words and Phrases and their Compositionality\" [Mikolov et al. 2013](https://arxiv.org/pdf/1310.4546.pdf) started a huge revolution. They used a **very simple neural model** with __one hidden layer__ for the prediction of next words / contexts. Their predictive model was **\"streaming compatible\"**, meaning it only __needed one minibatch__ of data __at a time__, to __iteratively approxiamte__ the __distribution of language__, thus could scale to __enormous corpora__ (in the range of 1-100 billion tokens!). \n",
    "\n",
    "The __quality__ of the learned vector representations were also __impressive__ - later on that below.\n",
    "\n",
    "\n",
    "It is important to emphasize, that later on some [research](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) showed that word2vec effectively learns **a good approximation of some factorization of the co-occurence space**, thus it is a highly effective substitute to counting based models.\n",
    "\n",
    "It is also noteworthy, that it had a very limited notion of context. \n",
    "\n",
    "Remember the \"1-skip-4-grams\"? \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*yiH5sZI-IBxDSQMKhvbcHw.png\" width=45%>\n",
    "\n",
    "This was originally a word2vec illustration.\n",
    "\n",
    "The __limitation of context__ will be held against it later on.\n",
    "\n",
    "\n",
    "\n",
    "### \"Blueprint\"\n",
    "\n",
    "The general blueprint of the word2vec model is as follows:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/rohan-varma/paper-analysis/master/word2vec-papers/models.png\" width=60%>\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/igSuE.png\" width=30%>\n",
    "\n",
    "\n",
    "According to Mikolov et al.:\n",
    "\n",
    "\"__Skip-gram__: works well with __small amount of the training data__, represents well __even rare words or phrases__.\n",
    "__CBOW__: several times __faster to train__ than the skip-gram, slightly __better accuracy__ for the __frequent words__.\"\n",
    "\n",
    "### Advantage\n",
    "\n",
    "These types of models, generally called __\"neural word embeddings\"__, since we __\"embed\"__ or __transform__ the __input__ words __into dense vector representations__ are __not used for the task__ they are __trained for__, which is predicting the next word in a minimal window context (though this can be the case in predictive text input or sentence generation).\n",
    "\n",
    "Their main advantage is their **internal, \"latent\" representation** about the semantic space. Typically the softmax layer for outputting next word predictions is completely discarded, only the \"mapping\" layer is being kept.\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1heogQhMfvtiOSfPtKvmc2OtyGadAsOmd\" width=60%>\n",
    "\n",
    "\n",
    "A good analysis can be found [Marek Rei's blogpost](http://www.marekrei.com/blog/dont-count-predict/).\n",
    "\n",
    "(Naturally the advance of these models did not stop, some of their more elaborate forms can be found [here](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a).)\n",
    "\n",
    "### Why can the inner representations be meaningful?\n",
    "\n",
    "The fact, that the __inner representation space__ of word2vec seems thoroughly meaningful points towards one of the __general phenomena underpinning modern machine learning__:\n",
    "\n",
    "_For the model to __solve the predictive task__ it was given, it had to __\"store\" and \"compress\"__ the __information__ inside the corpus so that it __fits through the \"bottleneck\"__, the __limited capacity__ of the model. The __best compression__ of the data is to capture their __\"salient features\"__, that is in this case their __morphologic and semantic characteristics__, their __\"form\" and \"meaning\"__. Thus a __systematic mapping arises__._\n",
    "\n",
    "\n",
    "Optimal compression, understanding and intelligence has deep relations, see: [Hütter prize](https://en.wikipedia.org/wiki/Hutter_Prize).\n",
    "\n",
    "For more on information bottleneck see [Tishby et al.'s work](https://en.wikipedia.org/wiki/Information_bottleneck_method).\n",
    "\n",
    "We can say, that the \"side effect\" of prediction is here the main result.\n",
    "\n",
    "**Good representation is all!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of rolling window models\n",
    "\n",
    "### Sidenote: GloVe was for a long time competitive!\n",
    "\n",
    "<img src=\"https://adriancolyer.files.wordpress.com/2016/04/glove-vs-word2vec.png\" width=55%>\n",
    "\n",
    "### Vocabulary wars\n",
    "\n",
    "#### Speed\n",
    "\n",
    "Important to note that since the __vocabulary__ in case of these predictive models is __large__, the $v$ vocabulary width layers were consuming __extreme amount of computation__ (by 2013 standards). (Think: a vocabulary of 300k).\n",
    "\n",
    "When the __linear layer__ is __very wide__ (e.g., there are a lot of classes in a classification task) __computing__ the full __softmax__ is __expensive__ because even to compute the probability of single output the exponentials of all outputs has to be computed. \"Cheaper\" alternatives had been developed, e.g. \n",
    "\n",
    "#####  Hierarchic Softmax\n",
    "\n",
    "Here we treat the softmax layer as a binary classification tree for a multiclass classification problem, and the linear outputs are at the internal nodes of the tree, where they are interpreted as the logarithms of odds for the classes on the left and right sides of the corresponding subtree:\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/L6siJ.png\" width=45%>\n",
    "\n",
    "To calculate the probability corresponding to an individual class one simply multiplies the probabilities belonging to the nodes on the path, e.g. for class $w_2$\n",
    "\n",
    "$$P(w_2) = (1-\\sigma(n(w_2,1)))(1-\\sigma(n(w_2,2)))\\sigma(n(w_2,3))$$\n",
    "\n",
    "$$= \\sigma(-n(w_2,1))\\sigma(-n(w_2,2))\\sigma(n(w_2,3))$$\n",
    "\n",
    "In general, it is enough to compute $\\log V$ exponentials to calculate the probability of a single item in the layer.\n",
    "\n",
    "Since (as we will soon see) during training it's typically enough to calculate the probability of one item, this can significantly speed up training. In contrast, there is typically no speedup for prediction, when all probabilities need to be calculated.\n",
    "\n",
    "See [Ruder's summary](http://ruder.io/word-embeddings-softmax/) about other alternatives.\n",
    "\n",
    "\n",
    "#### Rigidity\n",
    "\n",
    "As already mentioned in the section about transfer learning, one of the crucial __problems__ in __language model transfer__ is the potential existence for a __no-overlapping__ part of __vocabulary__, whereby some words present in the corpus are simply missing from the model. \n",
    "\n",
    "It is important to understand, that this case does not just effect transfer scenarios, it can very well be present in standard use cases, since **we can not guarantee that all words encountered in production were already present in the trainig set!** They are simply **out of vocabulary** (OoV)\n",
    "\n",
    "Some solutions trying to mitigate this problem are:\n",
    "\n",
    "##### Explicit Out of Vocabulary (OoV) token\n",
    "\n",
    "The default solution for this is to add a __general OoV__ token to the vocabulary by default, so as to forego this problem. Many times the **\"UNK\" token** is used for this purpose.\n",
    "\n",
    "The advantage of this approach is, that we can handle the case of __rare words inside the corpus__, that we __explicitly don't want to include in the model__ just in the __same way__, replacing them with UNK.\n",
    "\n",
    "The disadvantage though is, that we definitely __loose information__ on these tokens, which frequently are (new) proper names, thus are crucial eg. for search scenarios. (If we replace all the customer names to UNK, that would definitely not help document recall for sales...) \n",
    "\n",
    "##### Dedicated OoV model\n",
    "\n",
    "A more thorough approach to this problem is the usage of a dedicated __OoV model__ which gets as __input the word's surface form__ as well maybe a __\"list of brothers\" (potential related words)__ and has __to produce a word vector as prediction output__. Intuitively we could argue, that this approach works best if the words are __morphological variants__ of each-other.\n",
    "\n",
    "See an example of such models [here](https://hal.archives-ouvertes.fr/hal-01623784/document).\n",
    "\n",
    "Beyond these approaches the approach of **\"subword embeddings\"** emerged as a preferred choice in practice. See more on that later.\n",
    "\n",
    "### The problem of word senses\n",
    "\n",
    "As already mentioned before, it is a problem in all word embeddings, that the __different senses__, like (like bank(1) and bank(2)) are being __treated in a mixed manner__, since the \"key\", the vocabulary entry subsumes under it all the different usages and contexts.\n",
    "\n",
    "For a more explicit treatment of the homonymy phenomena we can try to capitalize on the fact that __homonyms__ are __sometimes__ (though definitely _not always_) come __from different parts of speech__, thus if we can __separate__ the __vocabulary__ entries __by parts of speech__, relying on a POS tagger as an external resource, we can build up more fine grained representations. That is exactly what [sense2vec](https://arxiv.org/abs/1511.06388) and others, like [here](https://www.cs.rochester.edu/~lsong10/papers/area.pdf) and [here](http://www.aclweb.org/anthology/Q15-1017) try to achieve.\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/36f9886ad1cb9ee3f66c5af0282ae7a3359b86b2/3-Figure2-1.png\" width=45%>\n",
    "\n",
    "But further on there is literature which proposes, that the word senses are represented quite well in the components of word vectors, even to the point that [detection of polysemy](https://arxiv.org/abs/1709.08858) or [explicit capitalization on included word senses](http://www.offconvex.org/2018/09/18/alacarte/) is possible.\n",
    "\n",
    "Example:\n",
    "\"tie  only has two meanings (clothing and game), it is easy to see that its word embedding is a linear transformation of the sum of the average context vectors of its two senses:\"\n",
    "\n",
    "$$v_w=A\\mathbb{E}v_w^\\textrm{avg}=A\\mathbb{E}\\left[v_\\textrm{clothing}^\\textrm{avg}+v_\\textrm{game}^\\textrm{avg}\\right]=A\\mathbb{E}v_\\textrm{clothing}^\\textrm{avg}+A\\mathbb{E}v_\\textrm{game}^\\textrm{avg}$$\n",
    "\n",
    "[(source)](http://www.offconvex.org/2018/09/18/alacarte/)\n",
    "\n",
    "### The problem of longer context\n",
    "\n",
    "The biggest problem though with rolling window models is that they __enforce the same Markov assumption__ as the n-gram models: they think in a __fixed context window__, are unable to model dependencies in a broad context. So we are back to square one.\n",
    "\n",
    "We need full sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Full sequence\" models\n",
    "\n",
    "One of the ways to circumvent the Markov assumption is to use such neural models. specificly recurrent neural networks that are capable of modeling potentially long range dependencies. We consider text as a long sequential data and use the appropriate architectures to predict next elements in the sequence:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1y8QYr9ftTvXAxgzS-ldnGlijVpmK2l21\" width=55%>\n",
    "\n",
    "Some things to note:\n",
    "\n",
    "- Input is a one-hot vector for the model, and **we use an embedding to transfrom it to a dense vector** (hint pretrained representations?)\n",
    "- Output is softmax over the vocabulary. We can use tricks for that...\n",
    "- Here a simple RNN is displayed, but LSTMs and other variants can (and should) be used.\n",
    "\n",
    "## Ok, but what is an RNN?\n",
    "\n",
    "Please consult Optional material if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching RNN-s on language data\n",
    "\n",
    "_In theory_ an RNN could be trained with full GD on the corpus in one go:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1XsBoRp7cNay3svFLRDv2JEDyC7m7CUdC\" width=50%>\n",
    "\n",
    "- The loss is generally the well-kown crossentropy, which is in this case (since the input is a one-hot vector):\n",
    "  $$J^{(i)}(\\Theta) = -\\log (\\hat y[x^{(i+1)}])$$\n",
    "  the negative logarithm of the probability assigned by the network to the right word / next word.\n",
    "\n",
    "- For the sake of more frequent updates, and since BPTT for long sequences is very expensive, teaching is done in smaller units with not necessarily the same length.\n",
    "- The unit is typically one or more sentece, or if the length allows, and we have enough material, a paragraph can be a good candidate.\n",
    "- Initial state in case of the time-series units: if the boundaries are inside a unit of text, it is important to _transfer the hidden state_ from the previous unit, in other cases initialization can be done by some fixed value.\n",
    "- (Somewhat misleading) terminology: the length of the \"time\" unit is _time step_, but sometimes certain implementations call it _minibatch_, though that would generally mean the number of units processed in one go for the sake of computaitonal efficiency.\n",
    "\n",
    "### Parameters of LSTM-architektures\n",
    "\n",
    "+ An LSTM is a complete layer! The most important parameter of it is the \"number of (memory) units\", which is the length of the hidden state vector, thus, the memory capacity.\n",
    "+ It is quite widespread to use multiple LSTM layers (\"stacked LSTMs\") -- as in the case of ConvNets the hope is, that the layers learn a hierarchy of abstracti representations:\n",
    "\n",
    "<img src=\"http://wenchenli.github.io/assets/img/GNMT_residual.png\" width=600 heigth=600>\n",
    "\n",
    "(on the right side a network is shown with skip/residual connections!)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Are LSTM-s really necessary?\n",
    "\n",
    "Please consult Optional material about ConvNets over sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some words about usage of language models\n",
    "\n",
    "Recap: But what is a language model good for?\n",
    "\n",
    "For example:\n",
    "- Predictive text input (\"autocomplete\")\n",
    "- Generating text\n",
    "- Spell checking\n",
    "- Language understanding\n",
    "- And most importantly representation learning \n",
    "\n",
    "## Generating text with a language model\n",
    "\n",
    "The language model produces a tree with probable continuations of the text:\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-Jjpb7iyB37A/WBZI4ImGQII/AAAAAAAAA9s/ululnUWt2vw9NMKuEr-F9H8tR2LEv36lACLcB/s1600/prefix_probability_tree.png\" width=50%>\n",
    "\n",
    "Using this tree we can try different algorithms to search for the best \"continuations\". A full breadth-first search oi usually impossible, due to the high branching factor of the tree.\n",
    "\n",
    "Alternatives:\n",
    "- \"Greedy\": we choose the continuation which has the highest direct probability, This will most probably be suboptimal, since the probability of the full sequence is tha product of the continuations, and if we would have chosen a different path, we might ahve been able to choose later words with hihg probabilities.\n",
    "- Beam-search: we always store a fixed $k$ number of partial sequences, and we always try to expand these, always keeping the most probable $k$ from the possible continuations. \n",
    "\n",
    "Example ($k$=5):\n",
    "\n",
    "<img src=\"http://opennmt.net/OpenNMT/img/beam_search.png\" width=50%>\n",
    " \n",
    " \n",
    "\n",
    "## Scaling up sequence models to larger chunks of text\n",
    "\n",
    "The big question is, that even when we have good quality representations for words in dense vector form, how can we create such mapping for higher levels inside the textual hierarchy, like sentences, paragraphs and documents?\n",
    "\n",
    "This also opens up two sub questions:\n",
    "1. (How) can we keep the representation of more complex elements in the same space as the one for words?\n",
    "2. How much we can learn about long chunks of texts at all? If we compress a book into a sentence, does it make sense?\n",
    "\n",
    "\n",
    "There are more \"direct\" and \"indirect\" methods for obtaining representations for longer chunks of texts.\n",
    "\n",
    "\n",
    "\n",
    "### \"Simply add it up\"\n",
    "\n",
    "**\"Bag-of-word-vectors\"**\n",
    "\n",
    "We simply take all the word vectors in a unit of text and we calculate some kind of \"average\".\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/wBu7G.png\" width=35%>\n",
    "\n",
    "And with this we get back to problems (and techniques) of BOW.\n",
    "\n",
    "### \"Use some weighting over it!\"\n",
    "\n",
    "We have learned from BOW that we would not like to let all words contribute equally to the representation, so we can use whatever we have learned by \"normalization\": POS, TFIDF, TextRank,...\n",
    "\n",
    "In short: \"Multiply by something, than average.\"\n",
    "\n",
    "\n",
    "### A very strong \"baseline\" - SIF\n",
    "\n",
    "\"A Simple but Tough-to-beat baseline for sentence embeddings\" - Sanjeev Arora, Yingyu Liang, Tengyu Ma, ICLR 2017\n",
    "\n",
    "[Forrás](http://www.offconvex.org/2018/06/17/textembeddings/)\n",
    "[Implementáció](https://github.com/PrincetonML/SIF )\n",
    "[Eredeti cikk](https://openreview.net/pdf?id=SyK00v5xx)\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/544269/41765307-a3e1392a-763e-11e8-9804-5eefc11f9459.png\" width=55%>\n",
    "\n",
    "This quite recent model is in a sense \"averaging on steroids\", where we use a hyperparameter to tune the averaging based on corpus frequency of words, then do SVD and remove the first principal component of the representation. \n",
    "\n",
    "This is hypothetized to be kind of noise filter, but... \n",
    "\n",
    "Empirics prevail.\n",
    "\n",
    "It is indeed VERY competitive with more complex methods.\n",
    "\n",
    "\n",
    "### \"Calculate it separately\"\n",
    "\n",
    "\n",
    "#### Topic + RNN models\n",
    "\n",
    "Three different approaches for combining sequence and topic models came out during 2016-17:\n",
    "[TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency](https://arxiv.org/abs/1611.01702)\n",
    "[Topically Driven Neural Language Model - TDLM](https://arxiv.org/abs/1704.08012)\n",
    "[Topic Compositional Neural Language Model](https://arxiv.org/abs/1712.09783)\n",
    "\n",
    "Common in them is, that beside the predictive LSTM model they also use a topic model as a kind of mixture, that is: they somehow mix in the predictions of the topic part in the final output for next element prediction - like concatenating the topic model's output to the input for LSTMs (or in different complicated ways...)\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/ef3971b8e822820d583edb0ed76a39647b18577c/2-Figure1-1.png\" width=75%>\n",
    "\n",
    "The main idea is, that it is true in parallel that the production probabilities are influenced by the sequence and some kind of topic mixture.\n",
    "\n",
    "There are newer, even more complex models in this direction, like [this](https://arxiv.org/abs/1810.03947).\n",
    "\n",
    "\n",
    "**Takeaway:**\n",
    "\n",
    "The topic RNN models try to answer the question of building multi layered representations of text in one model. the big question will be, if we can do this in a more flexible way?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging ontologies and prediction\n",
    "\n",
    "We have already seen, that statistical / predictive methods can be used to assist in building up ontologies, the question is, if ontologies can be of any use for predictive models themselves or is any kind of \"merger\" necessary.\n",
    "\n",
    "The problem is in itself complex, due to the different nature of the two representations: while graphical knowledge representations are **discrete**, that is, one atomic unit of them represents a \"piece\" of knowledge, vector representations are **\"global\"** or distributed, meaning their global \"state\" is encoding the knowledge, we can not pinpoint any single element as a \"carrier\". Their differing nature makes merging the two a great challenge.\n",
    "\n",
    "## Knowledge graphs as constraints\n",
    "\n",
    "### \"Retrofitting\"\n",
    "\n",
    "One of the most basic form of using knowledge graph information is to modify vector spaces \"after the fact\" based on the relations captured, that is, use a kind of vector transformation based on individual observed relations in a knowledge base to transform the vectors of two given words eg. closer to each other based on synonymy by an additional postprocessing optimization step.  \n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1E64Wadb_X1Ufqj9iHdMb-uz2yWB0jtB6\" width=85%>\n",
    "\n",
    "The two important papers describing this approach are [this](https://arxiv.org/abs/1411.4166) and [this](https://arxiv.org/abs/1603.00892), latter one also includes direct sysnonymy and antonymy information.\n",
    "\n",
    "Some discussion of \"retrofitting\" can be found [here](https://becominghuman.ai/enriching-word-vectors-with-lexicon-knowledge-and-semantic-relations-an-efficient-retrofitting-bcb5f1208a3e). \n",
    "\n",
    "There are quite current postprocessing methods like [\"extrofitting\"](https://arxiv.org/abs/1808.07337) which are also noteworthy.\n",
    "\n",
    "### \"Conditioning on...\"\n",
    "\n",
    "Hierarchic representations of knowledge can be understood as having a distinct topology of top-down structure. During buildup of vector spaces we can think of the this topology, that we would like to enforce upon the final vector space representation of the semantic space we are currently learning.\n",
    "\n",
    "A [Poincaré disk](https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model) can be thought of as a special topology that that enforces the vector space to lie inside a unit disc, distances conforming to the restrictions inside this space.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Poincare_disc_hyperbolic_parallel_lines.svg/600px-Poincare_disc_hyperbolic_parallel_lines.svg.png\" width=55%>\n",
    "\n",
    "The paper describing [Poincaré embeddings](https://arxiv.org/abs/1705.08039) is using this topology to represent hierarchical semantic structure in word2vec like models.\n",
    "\n",
    "The resulting vector models encode all semantic relatedness information but lay special emphasis on **learned hierarchic relations**. \n",
    "\n",
    "<img src=\"https://s3.ap-south-1.amazonaws.com/techleerimages/8435c41a-a112-41f7-88c0-b8edb0d4365c.jpg\" width=55%>\n",
    "\n",
    "Further upstream tasks could benefit from this kind of structure.\n",
    "\n",
    "A good discussion of Poincaré embeddings can be found [here](https://s3.ap-south-1.amazonaws.com/techleerimages/8435c41a-a112-41f7-88c0-b8edb0d4365c.jpg).\n",
    "\n",
    "## Knowledge graphs as additional sources of input\n",
    "\n",
    "As mentioned earlier, the recent emergence of neural models capable of handling graph based input, like graph-convolutional and graph-recursive neural networks raises to possibility of including graph based information in a (quasi) end-to-end manner to the training of neural models. One [recent work](https://arxiv.org/pdf/1902.07282.pdf) done in neural machine translation specifically uses semantic labeling information - produced by a separate model - to enhance the performance of an attentive model (see later) and thus give a boost to translation performance.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ysmQSLfBjU9y39BVJNXCSxh0pBjTKlEG\" width=85%>\n",
    "\n",
    "With this approach the authors were able to enhance the state of the art in NMT.\n",
    "\n",
    "\n",
    "With the recent performance gain of semantic taggers (like Google's [SLING](https://ai.googleblog.com/2017/11/sling-natural-language-frame-semantic.html)) this direction seems more and more viable as a kind of \"pipeline\" approach to enhancing final performance of models.\n",
    "\n",
    "## Direct ontology reasoning with neural models\n",
    "\n",
    "A noteworthy [recent paper](https://arxiv.org/pdf/1808.07980.pdf) even tries to completely \"subsume\" the question of ontological reasoning over graphs.\n",
    "\n",
    "As the authors write:\n",
    "\n",
    "\"The ability to conduct logical reasoning is a fundamental aspect of intelligent behavior, and thus an important problem along the way to human-level artificial intelligence. Traditionally, symbolic logic-based methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. More recently, however, there has been an increasing interest in using machine learning rather than symbolic logic-based formalisms to tackle these tasks. In this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. This is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. We present the outcomes of several experiments, which show that our model learned to perform precise ontology reasoning on diverse and challenging tasks. Furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view.\"\n",
    "\n",
    "If these models prove to be widely applicable, that would open up new possibilities in merging logical reasoning with distributed models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
