{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAQYiUIR6X_5"
   },
   "source": [
    "## Why BERT? \n",
    "\n",
    "- BERT stands for <code style=\"background:yellow;color:black\">Bidirectional Encoder Representations from Transformers</code>. It is designed to pre-train deep __bidirectional representations__ from unlabeled text by jointly conditioning on both left and right context.\n",
    "\n",
    "- Due to being bilingual it can generalises well and when trained on huge corpus it generates good results.\n",
    "\n",
    "- This __pre-training step__ is half the magic behind BERTâ€™s success. This is because as we train a model on a large text corpus, our model starts to pick up the deeper and intimate understandings of how the language works.\n",
    "\n",
    "\n",
    "\n",
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnTX7UneECXv",
    "outputId": "3c6b09a1-246c-4917-a820-dc14de1aa5fd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "import ktrain\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "23KJOfTpdfkE"
   },
   "outputs": [],
   "source": [
    "# Reading in proper format from our mentioned file\n",
    "df_train = pd.read_pickle('train_features.pkl')\n",
    "df_test  = pd.read_pickle('test_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "<b>BERT : </b>We will be using __KTrain Wrapper__ for it. Since the architecture of the model is predetermined we don't have a pre-defined model for it. So it's just to check how well it performs in comparision to my other models.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RFkQ6UKdeFY"
   },
   "source": [
    "## BERT Model\n",
    "\n",
    "The model used here is a pre-trained model by using K-train wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "hjAqryqFEnBP",
    "outputId": "73fd85f3-06c8-47e7-b227-5b6b5ffe3385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not_label', 'label']\n",
      "   not_label  label\n",
      "0        1.0    0.0\n",
      "1        1.0    0.0\n",
      "2        1.0    0.0\n",
      "3        0.0    1.0\n",
      "4        1.0    0.0\n",
      "['not_label', 'label']\n",
      "   not_label  label\n",
      "0        1.0    0.0\n",
      "1        1.0    0.0\n",
      "2        1.0    0.0\n",
      "3        1.0    0.0\n",
      "4        1.0    0.0\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test), preproc = ktrain.text.texts_from_df(train_df=df_train,\n",
    "                                                                   text_column = 'Text',\n",
    "                                                                   label_columns = 'label',\n",
    "                                                                   val_df = df_test,\n",
    "                                                                   maxlen = 512,\n",
    "                                                                   preprocess_mode = 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bl7KEUeSFBLA",
    "outputId": "7c2dee8d-41a7-4dcc-de4b-6ba57aa29932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "maxlen is 512\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "model = ktrain.text.text_classifier(name = 'bert',\n",
    "                             train_data = (X_train, y_train),\n",
    "                             preproc = preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RNGcK1omQ6F_"
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "learner = ktrain.get_learner(model=model, train_data=(X_train, y_train),\n",
    "                   val_data = (X_test, y_test),\n",
    "                   batch_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuhBig3AF_H1",
    "outputId": "ddb07cd9-9103-4628-afca-24c9efd0d2ce"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's train the model\n",
    "learner.fit_onecycle(2e-5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_2iT9Lo-kSN"
   },
   "outputs": [],
   "source": [
    "learner.view_top_losses(n=1, preproc=preproc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
